<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <base href="/lab-book/" target="_self">
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <!-- This line causes strange scaling on smart phone views, perhaps a better option is available than removal? -->
  <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /> -->
      <meta name="author" content="Aaron S. Wolf" />
        <meta name="dcterms.date" content="2021-02-06" />
    
  <title>Data weights for meaningful models: inferring rock-formation histories</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
      <link rel="stylesheet" href="templates/main.css" />
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
    <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  </head>

<body>
      <!--
      <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
      <html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
      <head>
          <meta http-equiv="content-type" content="text/html; charset=utf-8" />
          <meta name="description" content="Your description goes here" />
          <meta name="keywords" content="your,keywords,goes,here" />
          <meta name="author" content="Your Name" />
          <title>Aaron S. Wolf</title>
      </head>
      <body> -->

          <!-- <link rel="stylesheet" type="text/css" href="/css/main.css" title="andreas08" media="screen,projection" /> -->

      <div id="container" >
          <div id="header">
              <h1>aswolf LabBook</h1>
              <h2>email: aswolf _at_ umich _dot_ edu</h2>
          </div>

          <div id="navigation">
              <ul>
                  <li><a href="index.html">Index</a></li>

                  <li><a href="projects.html">Projects</a></li>

                  <!-- <li><a href="https://docs.google.com/spreadsheets/d/18hnjkcF4LSEzdFdd0fABXo3-bmSWMSfJb9io6kKTDGw/edit#gid=0">Daily-Tracker</a></li> -->
                  <li><a href="writing-tracker.html">Daily-Tracker</a></li>


                  <li><a href="writing-workshop.html">Writing-Workshop</a></li>

                  <li><a href="https://aswolf.github.io">Webpage</a></li>
              </ul>
          </div>

          <div class="content">
  
  <!-- Add post and post-header here -->

  <!-- <div class="post"> -->
    <!-- <header class="post-header"> -->
      <header id="title-block-header">
      <h1 class="title">Data weights for meaningful models: inferring rock-formation histories</h1>
      
      <br>
      <h2> &nbsp;&nbsp;&nbsp;
         Aaron S. Wolf 
        &nbsp;&nbsp;
         (2021-02-06) 
      </h2>
    </header>
  
  <!-- Related post info here -->
  <class="post-meta">
    <h4> Related Posts: </h4>

    <h4>       &nbsp;&nbsp;&nbsp; •<em> Previous: </em>
      <a href="posts/2021-02-03-rock-fitness-metric.html">
        Quantitative modeling of rock formation histories</a>
     </h4>

    <h4>  </h4>

  </class>
  <!-- </header> -->

  <br>

  <!-- motivation here -->
      <div class="box" markdown="1">
      <h2>Context & Motivation</h2>
        <b> <em> In most geological applications, the primary source of uncertainty is not measurement or analytical error, but rather errors generated by imperfect models or poorly constrained modeling conditions. Under these conditions, the potential importance of relative weighting across data types cannot be overstated. Even models that cannot faithfully capture every aspect of the observations can be accurate enough to provide useful predictions and explanations for a whole host of phenomena. This situation pulls the model calibration in multiple opposing directions, attempting to compromise between conflicting training data. Models that recover their calibration data with greater accuracy can lessen the impact of this contradiction, but when model errors dominate, the tension between two or more solutions is guaranteed to exist. This tension is resolved in the statistical arena through relative weights on the observational data, effectively communicating to the model which data are more or less worthy of careful attention. Thus, embodied within the values of the error weighting terms is a subjective choice about which data are more or less important, more or less deserving of faithful reproduction. Through fitting or Monte Carlo simulation, model parameters are adjusted and evaluated based on which values best capture the data types deemed most important by their small error terms. As a consequence, the favored family of models will be biased toward those aspects of the training data assigned the strongest weights via small error terms. This outcome is unavoidable when simulations rely upon imperfect models (as is often the case), and thus we should devote serious attention to error analysis to ensure that our final results properly reflect our best understanding of both the data and model limitations. </em> </b>
    </div>
  

      <div class="box" markdown="1">
      <h2>Key Points</h2>
      <ul>
                  <li> Argument for the under-appreciated general importance of selecting relative data weights (or comparative errorbar sizes) when combining analysis of differing data types, especially when model limitations dominate residuals. </li>
                  <li> Basic iterative error-reweighting scheme to obtain sensible error estimates reflecting model limitations. </li>
                  <li> Accidental bias in model calibration can arise from unreasonable initial error assignments for model-dominated error terms; this can even happen using analytical errors, justifying a comprehensive sensitivity analysis. </li>
                  <li> Overview of Bayesian sensitivity analysis focused on error-model hyperparameters, which capture acceptable total deviations between data and model; testing (or averaging) a range of values limits model bias. </li>
                  <li> Calibration model errors are decomposed into 3 sources; natural variability, analytical uncertainty, and model bias-weight; two can be empirically determined, with researchers’s subjective judgement stored in bias weights </li>
              </ul>
    </div>
  

  <article class="post-content">
    <!-- content -->
    <!-- Motivation adapted from [[202102011633]] Importance of relative weights across data types -->
    <!-- ## Importance of relative weights across data types -->
    <!-- [[202102011633]] Importance of relative weights across data types -->
    <h2 id="empirical-assignment-of-modeling-errors">Empirical Assignment of Modeling Errors</h2>
    <!-- [[202102011652]] Empirical Assignment of Modeling Errors -->
    <p>The Bayesian statistical framework makes the tradeoffs in modeling distinct data types explicit through assignment of error-terms. A typical empirical-data approach is to initially adopt a reasonable set of errors (<span class="math inline">\(\sigma_0\)</span>)—often based on modeling experience or prior studies—and then iteratively rescale the overall errors within each data-type until average model residuals match the imposed errors within each data-type category: <span class="math display">\[
    \sigma^2_{t+1}(\{d\}) = \langle \Delta y_t^2(\{ d \})\rangle
    \]</span> where <span class="math inline">\(\Delta y_t\)</span> is the current estimate of model misfit (at calibration time <span class="math inline">\(t\)</span>) for a particular subset of the data <span class="math inline">\(\{d\}\)</span>, and <span class="math inline">\(\sigma_{t+1}\)</span> is the error update for the next iteration of the calibration process <span class="math inline">\((t+1)\)</span>. Convergence of model residuals and errorbar values implies that each data point deviates within <span class="math inline">\(\pm1\sigma\)</span> of its model value (on average), reflecting an error estimate that captures (by construction) the empirical deviations of data from model. This method is generally appropriate in cases where model-limitations dominate the error budget, as is the case for many geological modeling applications. This error-reweighting scheme explicitly encapsulates the typical (unstated) assumption that the final best-fit model is a useful representation of the processes that generated the data (complex rock formation history in the case of MCS) and that remaining deviations from the model can be treated as residual statistical fluctuations. Therefore, the errors determined through this method actually provide a fully quantified summary of the benefits and limitations of a particular set of modeling choices.</p>
    <h2 id="sensitivity-to-initial-assignment-of-model-dominated-errors">Sensitivity to initial assignment of model-dominated errors</h2>
    <!-- [[202102040630]] Sensitivity to initial assignment of model-dominated errors -->
    <p>One of the clear potential drawbacks of this empirical model-error assessment scheme is its dependence on initially assumed values. <!-- [[202102011652]] --> If a modeling exercise is dominated by measurement uncertainties (where all underlying processes that generated the data are directly and accurately captured), then no iteration is required and the fitting algorithm will immediately converge on a single (accurate) family of solutions. In the far more typical case of simulation using incomplete models, the model must strike a compromise between the conflicting evidence being presented by the data. In truth, this apparent conflict is resolvable (in principle) by combining a more sophisticated model and possibly a more complete dataset, but we are charged to make progress with the data and model in hand.</p>
    <p>This inherent compromise introduces an ambiguity into the modeling exercise, in which the researcher’s choices about how to weight the various data types injects an implicit bias toward one family of solutions over another. In some cases, such a bias is welcome as it stems from a priori knowledge about the system or deep physical insight about the plausibility of particular scenarios. In most cases, however, this bias is the accidental consequence of the initially assumed data weights; the iterative error update scheme helps to partially mitigate against this danger, but it remains easy to unwittingly fall into one family of solutions over another, perhaps without even realizing that a tradeoff was made. The most insidious example of this accidental bias occurs when analytical uncertainties are (seemingly reasonably) adopted for each measurement, but the system is actually dominated by model errors. In this case, the calibration will unreasonably tend to give more credence to data with the smallest analytical errors—ignoring that such accuracy is impossible to achieve simultaneously for all measurements given model limitations—and the resulting model will be hopelessly biased toward some data and be forced to give correspondingly little weight inappropriately to other data. Due to the zero-sum nature of data-weights in model calibration, a seemingly innocuous initial assignment of analytical errors will result in a lopsided compromise between the conflicting data that remains entirely hidden from the modeler. It is therefore critical to explore and resolve the issue of relative data-weighting through sensitivity analysis. <!-- [[202102050555]] --></p>
    <h2 id="bayesian-sensitivity-analysis-for-mixed-data-models">Bayesian sensitivity analysis for mixed-data models</h2>
    <!-- [[202102050555]] Bayesian sensitivity analysis for mixed-data models -->
    <p>In the Bayesian statistical framework, assumed values for model-dominated errors are viewed as additional parameters of the statistical model, and their impact on physical/chemical model results are explored through a systematic sensitivity analysis. The complete set of model parameters are thus composed of the parameters describing the physical processes being studied along with the additional statistical model parameters (or hyperparameters), describing the data error-model needed to holistically combine all measurement constraints on the system. A complete analysis of the system requires exploration of <strong>both</strong> the physical parameters as well as the statistical ones. In addition to sampling the range of plausible physical parameter values, hyperparameters controlling the error-model are also sampled within a range of reasonable values. Ideally, we can demonstrate that within these plausible ranges, either the model solution is not particularly sensitive to the assumed error-model or alternatively we can marginalize (or combine results through averaging) across plausible value ranges. To implement this procedure, we must therefore quantify our assumptions about the acceptable range of model deviations for each observational data type. Establishing appropriate values and bounds for these hyperparameters is a non-trivial task involving considerable exploration, as it codifies deep intuition and past experience with both the underlying data and the limitations of this class of physical/chemical models, but can be made simpler and more explicit by decomposing the different sources of model-variance for each data-type. <!-- [[202102020605]] --></p>
    <h2 id="decomposing-errors-into-variability-relative-weights">Decomposing errors into variability &amp; relative weights</h2>
    <!-- [[202102020605]] Decomposing errors into variability & relative weights -->
    <p>One of the most challenging aspects of assigning errorbars for model calibration to mixed data-types is that the exercise inherently involves comparing apples and oranges. The different data classes typically represent separate measurement techniques, using different sample preparation methods, observing disparate characteristics of the system, and have unique non-overlapping sensitivities to distinct subsets of the model parameters. While top-down direct assignment of errorbars mathematically accomplishes placing all measurements on a common statistical deviance scale, the researcher has almost no methodological basis for selecting the magnitudes of these errors. The errors themselves reflect a combination of analytical uncertainties, model limitations, and the researcher’s own biases toward faithfully reproducing some physical phenomena over others, depending on the intended uses for the resulting model. Selecting error magnitudes in this space is too difficult a task to expect quality results that do not obfuscate the choices and tradeoffs being made.</p>
    <p>To improve clarity and enable a more data-based approach, we decompose these errors into the separate contributing factors, exposing the choices being made and providing better guidance for value selection. We propose to split the error-model uncertainties <span class="math inline">\((\sigma_{tot})\)</span> into three sources: <span class="math display">\[
    \sigma_{tot}^2 = (\sigma_{var}^2 + \sigma_{meas}^2) / w_{bias}
    \]</span> where <span class="math inline">\(\sigma_{var}\)</span> is the natural variability of the quantity measured, <span class="math inline">\(\sigma_{meas}\)</span> is the analytical measurement uncertainty, and <span class="math inline">\(w_{bias}\)</span> is the overall (positive) model-bias weight. The relative importance of each data type is thus tuned by selecting model-bias weights above (or below) one, thereby shrinking (or inflating) errors to guide the calibration process. By decomposing the overall error weights <span class="math inline">\(\sigma_{tot}\)</span> into these three separate sources, we capture the primary causes for data-model disagreement, while separating largely objective error sources (<span class="math inline">\(\sigma_{var}\)</span> and <span class="math inline">\(\sigma_{meas}\)</span>) that can be determined empirically from the subjective model-bias term, which reflects the intentions and judgement of the researcher building the model. <!-- [[*TK1*]] --> <!-- [[*TK2*]] --></p>
  </article>

  <!-- future-work here -->

      <br>
    <div class="box" markdown="1">
      <h2>Future Directions</h2>
      <b> <em> In a future post, we will explore how the empirical error contributions can be determined based on data from large databases like EarthChem. We will additionally explore the confounding role of correlated errors and how best to account for their impacts and sidestep their negative influences. </em> </b>
    </div>
  
      
      	</div>
      	<div id="footer">
      		<p>&copy; 2020 Aaron S. Wolf | Template design by <a href="http://andreasviklund.com/">Andreas Viklund</a></p>
      	</div>

      	</div>
      <!-- </body> -->
      <!-- </html> -->
  </body>

</html>
